# -*- coding: utf-8 -*-
"""Untitled42.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jIQPPVwTndAkddTjcAjX24fDwkfiLvId
"""

# === Installation Commands (Colab-specific) ===
!pip install pygad
!pip install -U imbalanced-learn



!pip install lime
!pip install pytorch-tabnet

# === Standard Libraries ===
import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# === Scikit-learn Core ===
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, ClassifierMixin

# === Scikit-learn Models ===
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import auc as sk_auc

# === Scikit-learn Metrics & Tools ===
from sklearn.metrics import (
    make_scorer, confusion_matrix, classification_report, accuracy_score,
    precision_recall_curve, roc_curve, auc, f1_score, cohen_kappa_score, ConfusionMatrixDisplay
)
from sklearn.inspection import permutation_importance
from sklearn.manifold import TSNE
from sklearn import linear_model
from pygad import GA


# === XGBoost ===
import xgboost as xgb

# === PyTorch TabNet ===
from pytorch_tabnet.tab_model import TabNetClassifier
import torch

# === Keras ===
from tensorflow.keras.utils import to_categorical

# === Imbalanced-learn (SMOTE & Pipelines) ===
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline  # Για να μπει SMOTE σωστά
from sklearn.model_selection import GridSearchCV  # Σωστό GridSearchCV


# === Model Interpretation ===
import shap
import lime
import lime.lime_tabular

#===Time====
import time
from sklearn.model_selection import cross_val_score


# === Google Colab (if running in Colab) ===
from google.colab import drive



warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)

#=== Mount Google Drive (for Colab use) ===
drive.mount('/content/drive')



# === Load Dataset ===
data_path = '/content/drive/My Drive/Colab Notebooks/train.csv'
data = pd.read_csv(data_path)

# Separate features and target
features_train = data.drop(columns=['Class/ASD', 'ID'])
target_train = data['Class/ASD']

# Keep a backup of the original features
combined_features = features_train.copy()

print("Data loaded successfully.")
print("Features shape:", combined_features.shape)

# === Set Random Seeds for Reproducibility ===
def reset_random_seeds(seed=42):
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    random.seed(seed)

seed = 42
reset_random_seeds(seed)

# === 2. Data Cleaning ===

# Handle 'ethnicity'
combined_features['ethnicity'] = (
    combined_features['ethnicity']
    .str.strip().str.lower()
    .replace({'?': np.nan, 'others': 'other'})
)
most_common_ethnicity = combined_features['ethnicity'].mode()[0]
combined_features['ethnicity'] = combined_features['ethnicity'].fillna(most_common_ethnicity)

le_ethnicity = LabelEncoder()
combined_features['ethnicity'] = le_ethnicity.fit_transform(combined_features['ethnicity'])

# Handle 'country of residence'
le_country = LabelEncoder()
combined_features['contry_of_res'] = le_country.fit_transform(combined_features['contry_of_res'])

#Map binary features
binary_map = {'no': 0, 'yes': 1}
gender_map = {'f': 0, 'm': 1}

combined_features['used_app_before'] = combined_features['used_app_before'].map(binary_map)
combined_features['jaundice'] = combined_features['jaundice'].map(binary_map)
combined_features['austim'] = combined_features['austim'].map(binary_map)
combined_features['gender'] = combined_features['gender'].map(gender_map)

# Drop unused columns
if 'age_desc' in combined_features.columns:
    combined_features.drop('age_desc', axis=1, inplace=True)

# Handle 'relation' column
combined_features['relation'] = combined_features['relation'].replace({'?': np.nan, 'Others': 'other'})
most_common_relation = combined_features['relation'].mode()[0]
combined_features['relation'] = combined_features['relation'].fillna(most_common_relation)

le_relation = LabelEncoder()
combined_features['relation'] = le_relation.fit_transform(combined_features['relation'])

# Final Missing Value Check
missing_counts = combined_features.isnull().sum()
print("Missing values after preprocessing:")
print(missing_counts[missing_counts > 0])

#  Final Feature Matrix
X_full = combined_features
y_full = target_train
feature_names = X_full.columns.tolist()



def plot_confusion(y_true, y_pred, title, save_path=None):
    fig, ax = plt.subplots()
    ConfusionMatrixDisplay.from_predictions(
        y_true, y_pred, display_labels=["No ASD", "ASD"], ax=ax, cmap='Blues'
    )
    ax.set_title(title)
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')
    plt.show()
    plt.close()


def plot_roc_curve(y_true, y_score, title, save_path=None):
    fpr, tpr, _ = roc_curve(y_true, y_score)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(title)
    plt.legend(loc="lower right")
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')
    plt.show()
    plt.close()


def precision_recall(y_true, y_score, title, save_path=None):
    precision, recall, _ = precision_recall_curve(y_true, y_score)
    plt.figure()
    plt.plot(recall, precision)
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title(title)
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')
    plt.show()
    plt.close()


def run_tsne(X, y, title="t-SNE Visualization", seed=42, save_path=None):
    try:
        tsne = TSNE(n_components=2, random_state=seed)
        X_embedded = tsne.fit_transform(X)
        plt.figure(figsize=(6, 5))
        plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, cmap='coolwarm', s=15)
        plt.title(title)
        plt.xlabel("t-SNE 1")
        plt.ylabel("t-SNE 2")
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, bbox_inches='tight')
        plt.show()
        plt.close()
    except Exception as e:
        print(f"t-SNE error: {e}")


def get_feature_importance(model, feature_names, X=None, y=None):
    try:
        if hasattr(model, 'named_steps'):
            for step_name, step in model.named_steps.items():
                if hasattr(step, 'feature_importances_') or hasattr(step, 'coef_'):
                    model = step
                    break

        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
        elif hasattr(model, 'coef_'):
            importances = np.abs(model.coef_).flatten()
        elif hasattr(model, 'get_feature_importances'):
            importances = model.get_feature_importances()
        else:
            return None

        return pd.DataFrame({
            'Feature': feature_names,
            'Importance': importances
        }).sort_values(by='Importance', ascending=False)

    except Exception as e:
        print(f"Error in get_feature_importance: {e}")
        return None


def run_all_feature_importance_methods(model, X_train, X_test, y_train, y_test, feature_names,
                                       model_name="Model", output_dir="results", step=1):
    os.makedirs(output_dir, exist_ok=True)
    model_dir = os.path.join(output_dir, model_name.replace(" ", "_"))
    os.makedirs(model_dir, exist_ok=True)

    results = {}
    base_f1 = f1_score(y_test, model.predict(X_test), average='weighted')

    # Native Importance
    native_df = get_feature_importance(model, feature_names)
    if native_df is not None and not native_df.empty:
        native_df.to_csv(os.path.join(model_dir, "native_importance.csv"), index=False)
        results["native"] = native_df


    # PMC
    pmc_data = []
    for i in range(X_test.shape[1]):
        X_temp = X_test.copy()
        mean_val = X_temp.iloc[:, i].mean()
        X_temp.iloc[:, i] = mean_val
        f1_drop = base_f1 - f1_score(y_test, model.predict(X_temp), average='weighted')
        pmc_data.append((feature_names[i], f1_drop))
    pmc_df = pd.DataFrame(pmc_data, columns=['Feature', 'F1_Drop']).sort_values(by='F1_Drop', ascending=False)
    pmc_df.to_csv(os.path.join(model_dir, "pmc_importance.csv"), index=False)
    results["pmc"] = pmc_df

    # SOFI
    try:


        def feature_flip(model, X, y, permutation):
            scores = []
            X_copy = X.copy()
            for idx in permutation:
                X_copy.iloc[:, idx] = X_copy.iloc[:, idx].mean()
                preds = model.predict(X_copy)
                f1 = f1_score(y, preds, average='macro')
                scores.append(f1)
            return scores

        def fit_value(model, X, y, permutation):
            weights = np.arange(1.0, 0.0, -1 / (len(permutation) + 1))
            g_values = feature_flip(model, X, y, permutation)
            g_values = [base_f1] + g_values
            area = np.sum(weights * g_values)
            penalty = 0.1 * np.sum(np.maximum(np.diff(g_values), 0))
            return area + penalty

        def fitness_func(ga_instance, solution, solution_idx):
            area = fit_value(model, X_test, y_test, solution)
            return 1 / (area + 1e-6)

        sofi_ga = GA(num_generations=20,
                     num_parents_mating=20,
                     fitness_func=fitness_func,
                     sol_per_pop=100,
                     num_genes=X_test.shape[1],
                     gene_space=list(range(X_test.shape[1])),
                     allow_duplicate_genes=False,
                     mutation_probability=0.01,
                     gene_type=int)
        sofi_ga.run()
        sofi_permutation, _, _ = sofi_ga.best_solution()
        results["sofi_permutation"] = sofi_permutation
        np.save(os.path.join(model_dir, "sofi_permutation.npy"), sofi_permutation)

    except Exception as e:
        results["sofi_permutation"] = list(range(X_test.shape[1]))

    # PFI from sklearn
    try:
        from sklearn.inspection import permutation_importance
        pfi_output = permutation_importance(model, X_test, y_test, scoring='f1_weighted', n_repeats=30, random_state=42)
        pfi_permutation = np.argsort(-pfi_output.importances_mean).tolist()
        results["pfi_permutation"] = pfi_permutation
    except Exception as e:
        pfi_permutation = list(range(X_test.shape[1]))
        results["pfi_permutation"] = pfi_permutation
  # === SHAP permutation for flip curve (only for RF/XGB) ===
    try:
        estimator = model
        if hasattr(model, 'named_steps'):
            for name, step in model.named_steps.items():
                if hasattr(step, "predict_proba"):
                    estimator = step
                    break

        model_name_lower = model_name.lower()
        if "random forest" in model_name_lower or "xgboost" in model_name_lower:
            explainer = shap.TreeExplainer(estimator)
            shap_values = explainer.shap_values(X_test)

            if isinstance(shap_values, list):
                shap_values = shap_values[1]
            elif shap_values.ndim == 3:
                shap_values = shap_values[:, :, 1]

            mean_abs_shap = np.abs(shap_values).mean(axis=0)
            shap_permutation = np.argsort(-mean_abs_shap).tolist()
            results["shap_permutation"] = shap_permutation
    except Exception as e:
        results["shap_permutation"] = list(range(X_test.shape[1]))


    # Flip Curves (ONLY for Native, SOFI, PFI)
    def get_curve(permutation):
        g = feature_flip(model, X_test, y_test, permutation)
        return [base_f1] + g

    try:
        curves = {
            "Native": get_curve(list(range(X_test.shape[1]))),
            "SOFI": get_curve(results["sofi_permutation"]),
            "PFI": get_curve(results["pfi_permutation"]),


        }

        auc_results = {}
        for label, curve in curves.items():
            x = list(range(len(curve)))
            y = curve
            auc_score = np.trapz(y, x)
            auc_results[label] = auc_score

        # Save AUC results to a file
        auc_df = pd.DataFrame.from_dict(auc_results, orient='index', columns=["Flip Curve AUC"])
        auc_df.index.name = "Method"
        auc_df.to_csv(os.path.join(model_dir, "flip_curve_auc_scores.csv"))


        if model_name == "LTCN" and "ltcn_native_permutation" in results:
            curves["Native (LTCN)"] = get_curve(results["ltcn_native_permutation"])
        if "shap_permutation" in results:
          curves["SHAP"] = get_curve(results["shap_permutation"])

        plt.figure(figsize=(8, 6))
        for label, curve in curves.items():
            plt.plot(range(len(curve)), curve, marker='o', label=label)
        plt.title(f"Feature Removal Curves – {model_name}")
        plt.xlabel("Number of Marginalized Features")
        plt.ylabel("F1 Score")
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.savefig(os.path.join(model_dir, "flip_curves.png"), dpi=300)
        plt.show()
        plt.close()
    except Exception as e:
        pass



    # SHAP + LIME only for RF / XGBoost
    try:
        run_shap_and_lime(
            model_pipeline=model,
            X=X_test,
            feature_names=feature_names,
            model_name=model_name,
            class_names=["No ASD", "ASD"],
            save_dir=output_dir,
            seed=42
        )
    except Exception as e:
        pass

    return results


def run_shap_and_lime(model_pipeline, X, feature_names, model_name="Model",
                      class_names=["No ASD", "ASD"], save_dir="results", seed=42):
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    random.seed(seed)

    estimator = model_pipeline
    scaler = None
    if hasattr(model_pipeline, 'named_steps'):
        for name, step in model_pipeline.named_steps.items():
            if hasattr(step, 'predict_proba'):
                estimator = step
            if 'scaler' in name.lower():
                scaler = step

    if estimator is None:
        return

    X_input = scaler.transform(X) if scaler else X
    X_input = pd.DataFrame(X_input, columns=feature_names)
    X_sample = X_input.sample(n=min(100, len(X_input)), random_state=seed)

    model_name_lower = model_name.lower()
    if not ("random forest" in model_name_lower or "xgboost" in model_name_lower):
        return

    model_dir = os.path.join(save_dir, model_name.replace(" ", "_"))
    os.makedirs(model_dir, exist_ok=True)

    # SHAP
    try:
        explainer = shap.TreeExplainer(estimator)
        shap_values = explainer.shap_values(X_sample)

        if isinstance(shap_values, list):
            shap_values_to_plot = shap_values[1]
        elif shap_values.ndim == 3:
            shap_values_to_plot = shap_values[:, :, 1]
        else:
            shap_values_to_plot = shap_values

        if shap_values_to_plot.ndim == 1:
            shap_values_to_plot = shap_values_to_plot.reshape(-1, 1)

        shap_df = pd.DataFrame({
            "Feature": feature_names,
            "SHAP Value": np.abs(shap_values_to_plot).mean(axis=0)
        }).sort_values(by="SHAP Value", ascending=False)
        shap_df.to_csv(os.path.join(model_dir, "shap_summary.csv"), index=False)

        plt.figure()
        shap.summary_plot(shap_values_to_plot, X_sample.values, feature_names=feature_names, show=False)
        plt.title(f"SHAP Summary – {model_name}")
        plt.tight_layout()
        plt.savefig(os.path.join(model_dir, "shap_summary.png"), bbox_inches='tight')
        plt.show()
        plt.close()

        plt.figure()
        shap.plots.beeswarm(shap_values_to_plot, show=False)
        plt.savefig(os.path.join(model_dir, "shap_beeswarm.png"), bbox_inches='tight')
        plt.show()
        plt.close()
    except Exception as e:
        pass

    # LIME
    try:
        lime_explainer = lime.lime_tabular.LimeTabularExplainer(
            training_data=X_input.values,
            feature_names=feature_names,
            class_names=class_names,
            mode="classification",
            discretize_continuous=True
        )

        exp = lime_explainer.explain_instance(X_input.iloc[0].values, estimator.predict_proba, labels=[1])
        lime_html_path = os.path.join(model_dir, "lime_instance.html")
        exp.save_to_file(lime_html_path)

        try:
            exp.show_in_notebook(show_table=True, show_predicted_value=True)
        except Exception:
            pass
    except Exception as e:
        pass

def compare_feature_importance(native_df, shap_df=None, pmc_df=None, top_n=10, print_spearman=True):
    try:
        native_df = native_df.sort_values(by="Importance", ascending=False).head(top_n)
        combined = native_df[["Feature", "Importance"]].copy()
        combined.rename(columns={"Importance": "Native Importance"}, inplace=True)
        combined["Native Rank"] = range(1, len(combined) + 1)

        if shap_df is not None and not shap_df.empty:
            if "SHAP Value" not in shap_df.columns and "Importance" in shap_df.columns:
                shap_df = shap_df.rename(columns={"Importance": "SHAP Value"})

            combined = combined.merge(shap_df[["Feature", "SHAP Value"]], on="Feature", how="left")

        if pmc_df is not None and not pmc_df.empty:
            combined = combined.merge(pmc_df[["Feature", "F1_Drop"]], on="Feature", how="left")
            combined.rename(columns={"F1_Drop": "PMC Impact"}, inplace=True)


    except Exception as e:
        return pd.DataFrame()

def compare_all_importance_methods(model_name, output_dir="results", top_n=10):
    model_dir = os.path.join(output_dir, model_name.replace(" ", "_"))
    importance_data = {}

    try:
        native_df = pd.read_csv(os.path.join(model_dir, "native_importance.csv"))
        native_df = native_df.sort_values(by="Importance", ascending=False).head(top_n)
        importance_data["Native"] = native_df.set_index("Feature")["Importance"]
    except:
        pass

    try:
        perm_df = pd.read_csv(os.path.join(model_dir, "perm_importance.csv"))
        perm_df = perm_df.sort_values(by="F1_Drop", ascending=False).head(top_n)
        importance_data["Permutation"] = perm_df.set_index("Feature")["F1_Drop"]
    except:
        pass

    try:
        pmc_df = pd.read_csv(os.path.join(model_dir, "pmc_importance.csv"))
        pmc_df = pmc_df.sort_values(by="F1_Drop", ascending=False).head(top_n)
        importance_data["PMC"] = pmc_df.set_index("Feature")["F1_Drop"]
    except:
        pass

    try:
        shap_df = pd.read_csv(os.path.join(model_dir, "shap_summary.csv"))
        shap_df = shap_df.sort_values(by="SHAP Value", ascending=False).head(top_n)
        importance_data["SHAP"] = shap_df.set_index("Feature")["SHAP Value"]
    except:
        pass

    lime_path = os.path.join(model_dir, "lime_instance.html")
    if os.path.exists(lime_path):
        try:
            with open(lime_path, "r", encoding="utf-8") as f:
                html = f.read()
            import re
            lime_pairs = re.findall(r'\("(.+?)", ([\-\d.]+)\)', html)
            lime_dict = dict((feat.strip(), abs(float(val))) for feat, val in lime_pairs[:top_n])
            importance_data["LIME"] = pd.Series(lime_dict)
        except:
            pass

    combined = pd.DataFrame(importance_data).fillna(0)
    combined = combined.sort_values(by=combined.columns[0], ascending=False)

    combined_norm = combined.copy()
    for col in combined.columns:
        max_val = combined[col].max()
        if max_val > 0:
            combined_norm[col] = combined[col] / max_val

    try:
        plt.figure(figsize=(12, 6))
        combined_norm.head(top_n).plot(kind='bar')
        plt.title(f"Feature Importance Comparison – {model_name}")
        plt.ylabel("Normalized Importance")
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.grid(True, axis='y', linestyle='--', alpha=0.6)
        plt.savefig(os.path.join(model_dir, "importance_comparison.png"), dpi=300, bbox_inches='tight')
        plt.show()
        plt.close()
    except:
        pass

    return combined.head(top_n)


def plot_fairness_bar(y_true, y_pred, sensitive_attr, group_labels=None, positive_label=1,
                      title="Fairness by Group", save_path=None):
    groups = np.unique(sensitive_attr)
    fairness_dict = {}

    for g in groups:
        idx = (sensitive_attr == g)
        rate = (y_pred[idx] == positive_label).mean()
        label = group_labels[g] if group_labels else str(g)
        fairness_dict[label] = rate

    plt.figure(figsize=(6, 4))
    sns.barplot(x=list(fairness_dict.keys()), y=list(fairness_dict.values()))
    plt.ylim(0, 1)
    plt.ylabel("Positive Prediction Rate")
    plt.title(title)
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')
    plt.show()
    plt.close()

    return fairness_dict



class LTCN(BaseEstimator, ClassifierMixin): # ltcn
    def __init__(self, T=10, phi=0.8, method="ridge", function="sigmoid", alpha=1.0E-2):
        self.T = T
        self.phi = phi
        self.method = method
        self.alpha = alpha
        self.function = function

        self.W1 = None
        self.W2 = None
        self.model = None

    def fit(self, X_train, Y_train):
        self.classes_ = np.unique(Y_train)


        Y_encoded = to_categorical(Y_train, num_classes=len(self.classes_))

        if self.W1 is None:

            self.W1 = self.svd_based_initialization(X_train)
        H = self.reasoning(X_train)
        if self.method == 'inverse':

            self.W2 = np.linalg.pinv(self.add_bias(H)) @ Y_encoded
        elif self.method == 'ridge':

            self.model = linear_model.Ridge(alpha=self.alpha)
            self.model.fit(H, Y_encoded)


        return self



    def predict(self, X):
        if self.method == "inverse":
            Y_pred = self.add_bias(self.reasoning(X)) @ self.W2
            Y_pred = np.argmax(Y_pred, axis=1)
        elif self.method == "ridge":
            Y_pred = self.model.predict(self.reasoning(X))
            Y_pred = np.argmax(Y_pred, axis=1)

        return Y_pred

    def reasoning(self, A):
        A0 = A
        H = A0


        for t in range(self.T):
            A = self.phi * self.transform(A @ self.W1) + (1-self.phi) * A0
            H = np.concatenate((H, A), axis=1)


        return H


    def transform(self, X):

        return 1 / (1 + np.exp(-X)) if self.function == "sigmoid" else np.tanh(X)

    def add_bias(self, X):

        return np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)

    def svd_based_initialization(self, X):
        _, _, Vt = np.linalg.svd(X, full_matrices=False)


        weight_matrix = Vt.T # nxn

        return weight_matrix / np.max(np.abs(weight_matrix))


def evaluate_all_models(X_full, y_full, sensitive_attr_column='gender'):
    reset_random_seeds()

    all_feature_importance = []
    all_fairness = []
    all_model_scores = []
    model_times = {}
    evaluation_times = {}

    X_train, X_test, y_train, y_test = train_test_split(
        X_full, y_full, test_size=0.2, random_state=42, stratify=y_full
    )

    sensitive_attr_train = X_train[sensitive_attr_column].values
    sensitive_attr_test = X_test[sensitive_attr_column].values

    models = {}
    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

    model_defs = {
        "Logistic Regression": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('smote', SMOTE(random_state=42)),
                ('logisticregression', LogisticRegression(max_iter=1000))
            ]),
            {
                'logisticregression__penalty': ['l2'],
                'logisticregression__solver': ['saga'],
            }
        ),
        "Random Forest": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('smote', SMOTE(random_state=42)),
                ('randomforestclassifier', RandomForestClassifier(random_state=42))
            ]),
            {
                'randomforestclassifier__n_estimators': [100, 200],
                'randomforestclassifier__max_depth': [5, 10, None],
                'randomforestclassifier__min_samples_split': [2, 5, 10],
                'randomforestclassifier__min_samples_leaf': [1, 2, 4],

            }
        ),
        "Decision Tree": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('smote', SMOTE(random_state=42)),
                ('decisiontreeclassifier', DecisionTreeClassifier(random_state=42))
            ]),
            {
                'decisiontreeclassifier__criterion': ['gini', 'entropy'],
                'decisiontreeclassifier__max_depth': [3, 5, 10],
                'decisiontreeclassifier__min_samples_split': [2, 5, 10],
                'decisiontreeclassifier__min_samples_leaf': [1, 2, 4],
            }
        ),
        "TabNet": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('smote', SMOTE(random_state=42)),
                ('tabnetclassifier', TabNetClassifier(seed=42, verbose=0))
            ]),
            {
                'tabnetclassifier__n_d': [8, 16],
                'tabnetclassifier__n_a': [8, 16],
                'tabnetclassifier__n_steps': [3, 5],
                'tabnetclassifier__gamma': [1.0, 1.3],
                'tabnetclassifier__lambda_sparse': [1e-3, 1e-4]
            }
        )
        ,
        "XGBOOST": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('smote', SMOTE(random_state=42)),
                ('xgbclassifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))
            ]),
            {
                "xgbclassifier__max_depth": [2, 3, 4],
                "xgbclassifier__n_estimators": [50, 100],
                "xgbclassifier__learning_rate": [0.01, 0.05, 0.1],
                "xgbclassifier__subsample": [0.7, 0.8, 0.9],
                "xgbclassifier__gamma": [0, 0.1, 0.2]
            }
        ),
        "LTCN": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('smote', SMOTE(random_state=42)),
                ('ltcn', LTCN())
            ]),
            {
                'ltcn__T': [10, 15, 20, 25, 30, 35],
                'ltcn__phi': [0.5, 0.8, 0.85, 0.90, 0.95],
                'ltcn__method': ['inverse', 'ridge'],
                'ltcn__function': ['sigmoid', 'tanh'],
                'ltcn__alpha': [0.0001, 0.001, 0.01]
            }
        ),
    }

    for model_name, (pipeline, param_grid) in model_defs.items():
        print(f"\nTraining and tuning {model_name}...")
        start_time = time.time()

        grid = GridSearchCV(pipeline, param_grid, cv=cv, scoring='f1_weighted', n_jobs=-1)
        grid.fit(X_train, y_train)

        end_time = time.time()
        model_times[model_name] = round(end_time - start_time, 2)
        models[model_name] = grid
        model_dir = os.path.join("results", model_name.replace(" ", "_"))
        os.makedirs(model_dir, exist_ok=True)

        best_params = grid.best_params_
        params_path = os.path.join(model_dir, "best_hyperparameters.json")
        with open(params_path, "w") as f:
            import json
            json.dump(best_params, f, indent=4)

        print(f"Cross-validation scores for {model_name}...")
        best_model = grid.best_estimator_
        cv_scores = cross_val_score(best_model, X_train, y_train, cv=cv, scoring='f1_weighted', n_jobs=-1)

        model_dir = os.path.join("results", model_name.replace(" ", "_"))
        os.makedirs(model_dir, exist_ok=True)

        cv_df = pd.DataFrame({"Fold": list(range(1, len(cv_scores) + 1)), "F1 Score": cv_scores})
        cv_df.to_csv(os.path.join(model_dir, "per_fold_f1_scores.csv"), index=False)

        plt.figure(figsize=(8, 6))
        bars = plt.bar(cv_df["Fold"], cv_df["F1 Score"], color='skyblue')
        plt.xlabel("Fold")
        plt.ylabel("F1 Score")
        plt.title(f"{model_name} - Per-Fold F1 Scores")
        plt.ylim(0, 1.0)
        for bar in bars:
            yval = bar.get_height()
            plt.text(bar.get_x() + bar.get_width() / 2.0, yval + 0.01, f"{yval:.2f}", ha='center', va='bottom')
        plt.tight_layout()
        plt.savefig(os.path.join(model_dir, "per_fold_f1_plot.png"))
        plt.close()

        val_mean_f1 = grid.best_score_
        val_std_f1 = grid.cv_results_['std_test_score'][grid.best_index_]
        grid.best_estimator_._val_mean_f1 = val_mean_f1
        grid.best_estimator_._val_std_f1 = val_std_f1

    for name, model in models.items():
        start_eval = time.time()
        evaluate_model_performance(
            name=name,
            model=model,
            X_train=X_train,
            y_train=y_train,
            X_test=X_test,
            y_test=y_test,
            feature_names=X_full.columns.tolist(),
            sens_test=sensitive_attr_test,
            output_dir="results",
            feature_importances_collector=all_feature_importance,
            fairness_collector=all_fairness,
            score_collector=all_model_scores
        )
        end_eval = time.time()
        evaluation_times[name] = round(end_eval - start_eval, 2)

    if all_model_scores:
        score_df = pd.DataFrame(all_model_scores)
        score_df.to_csv("results/model_scores_summary.csv", index=False)
        print("\nModel score summary saved to results/model_scores_summary.csv")
        print(score_df.to_string(index=False))

    train_df = pd.DataFrame(list(model_times.items()), columns=["Model", "Training Time (s)"])
    train_df.to_csv("results/model_training_times.csv", index=False)

    eval_df = pd.DataFrame(list(evaluation_times.items()), columns=["Model", "Evaluation Time (s)"])
    eval_df.to_csv("results/model_evaluation_times.csv", index=False)

    final_df = pd.DataFrame(all_model_scores)
    final_df["Training Time (s)"] = final_df["Model"].map(model_times)
    final_df["Evaluation Time (s)"] = final_df["Model"].map(evaluation_times)
    final_df.to_csv("results/model_overall_summary.csv", index=False)
    print("\nFinal Model Summary saved to results/model_overall_summary.csv")
    print(final_df.to_string(index=False))

    plt.figure(figsize=(10, 6))
    final_df_sorted = final_df.sort_values(by="Test F1", ascending=False)
    plt.bar(final_df_sorted["Model"], final_df_sorted["Test F1"])
    plt.title("Model Comparison by Test F1 Score")
    plt.ylabel("Test F1 Score")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig("results/model_comparison_plot.png")
    plt.show()
    plt.close()

def evaluate_model_performance(name, model, X_train, y_train, X_test, y_test, feature_names, sens_test,
                               output_dir="results", feature_importances_collector=None,
                               fairness_collector=None, score_collector=None, curve_data_collector=None):


    model_dir = os.path.join(output_dir, name.replace(" ", "_").replace("(", "").replace(")", ""))
    os.makedirs(model_dir, exist_ok=True)

    if isinstance(model, GridSearchCV):
        model = model.best_estimator_

    val_mean_f1 = getattr(model, "_val_mean_f1", None)
    val_std_f1 = getattr(model, "_val_std_f1", None)

    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    train_f1 = f1_score(y_train, y_train_pred, average='weighted')
    test_f1 = f1_score(y_test, y_test_pred, average='weighted')
    train_kappa = cohen_kappa_score(y_train, y_train_pred)
    test_kappa = cohen_kappa_score(y_test, y_test_pred)

    if score_collector is not None:
        score_data = {
            "Model": name,
            "Train F1": round(train_f1, 4),
            "Train Kappa": round(train_kappa, 4),
            "Test F1": round(test_f1, 4),
            "Test Kappa": round(test_kappa, 4)
        }
        if val_mean_f1 is not None:
            score_data["Validation F1"] = round(val_mean_f1, 4)
            score_data["Validation F1 Std"] = round(val_std_f1, 4)
        score_collector.append(score_data)

    try:
        y_score = model.predict_proba(X_test)[:, 1]
    except:
        try:
            y_score = model.decision_function(X_test)
        except:
            y_score = y_test_pred

    if curve_data_collector is not None:
        curve_data_collector[name] = {'y_true': y_test, 'y_score': y_score}

    plot_confusion(
        y_test, y_test_pred,
        title=f"{name} - Confusion Matrix",
        save_path=os.path.join(model_dir, "confusion_matrix.png")
    )
    plot_roc_curve(
        y_test, y_score,
        title=f"{name} - ROC Curve",
        save_path=os.path.join(model_dir, "roc_curve.png")
    )
    precision_recall(
        y_test, y_score,
        title=f"{name} - Precision-Recall Curve",
        save_path=os.path.join(model_dir, "precision_recall.png")
    )

    def get_ltcn_feature_importance(ltcn_model, feature_names):
        try:
            W = ltcn_model.W1
            importances = np.sum(np.abs(W), axis=1)
            return pd.DataFrame({
                "Feature": feature_names,
                "Importance": importances
            }).sort_values(by="Importance", ascending=False)
        except:
            return None

    def plot_feature_importance(df, model_name, save_path=None):
        df_sorted = df.sort_values("Importance", ascending=True)
        plt.figure(figsize=(8, 6))
        plt.barh(df_sorted["Feature"], df_sorted["Importance"])
        plt.xlabel("Importance")
        plt.title(f"Feature Importance - {model_name}")
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path)
        plt.close()

    try:
        results = {}
        ltcn_model = model
        if hasattr(model, 'named_steps') and 'ltcn' in model.named_steps:
            ltcn_model = model.named_steps['ltcn']

        if hasattr(ltcn_model, "W1"):
            native_df = get_ltcn_feature_importance(ltcn_model, feature_names)
            if native_df is not None:
                native_df.to_csv(os.path.join(model_dir, "native_importance.csv"), index=False)
                results["native"] = native_df
                plot_feature_importance(native_df, model_name=name,
                                        save_path=os.path.join(model_dir, "feature_importance.png"))
                if feature_importances_collector is not None:
                    for row in native_df.itertuples(index=False):
                        feature_importances_collector.append({
                            "Model": name,
                            "Feature": row.Feature,
                            "Importance": round(row.Importance, 5)
                        })

        additional_results = run_all_feature_importance_methods(
            model=model,
            X_train=X_train,
            X_test=X_test,
            y_train=y_train,
            y_test=y_test,
            feature_names=feature_names,
            model_name=name,
            output_dir=output_dir
        )
        results.update(additional_results)

    except Exception as e:
        print(f"Feature attribution error in {name}: {e}")

    try:
        fairness_dict = plot_fairness_bar(
            y_true=y_test,
            y_pred=y_test_pred,
            sensitive_attr=sens_test,
            group_labels=["Male", "Female"],
            save_path=os.path.join(model_dir, "fairness.png")
        )
        if fairness_collector is not None:
            for group, rate in fairness_dict.items():
                fairness_collector.append({
                    "Model": name,
                    "Group": group,
                    "Positive Prediction Rate": round(rate, 3)
                })
    except Exception as e:
        print(f"Fairness error in {name}: {e}")

evaluate_all_models(X_full, y_full, sensitive_attr_column='gender')



def evaluate_all_models_WITHOUT(X_full, y_full, sensitive_attr_column='gender'):
    reset_random_seeds()

    all_feature_importance = []
    all_fairness = []
    all_model_scores = []
    model_times = {}
    evaluation_times = {}

    X_train, X_test, y_train, y_test = train_test_split(
        X_full, y_full, test_size=0.2, random_state=42, stratify=y_full
    )

    sensitive_attr_train = X_train[sensitive_attr_column].values
    sensitive_attr_test = X_test[sensitive_attr_column].values

    models = {}
    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

    model_defs = {
        "Logistic Regression": (
            Pipeline([
                ('scaler', StandardScaler()),

                ('logisticregression', LogisticRegression(max_iter=1000))
            ]),
            {
                'logisticregression__penalty': ['l2'],
                'logisticregression__solver': ['saga'],

            }
        ),
        "Random Forest": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('randomforestclassifier', RandomForestClassifier(random_state=42))
            ]),
            {
                'randomforestclassifier__n_estimators': [100, 200],
                'randomforestclassifier__max_depth': [5, 10, None],
                'randomforestclassifier__min_samples_split': [2, 5, 10],
                'randomforestclassifier__min_samples_leaf': [1, 2, 4],
            }
        ),
        "Decision Tree": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('decisiontreeclassifier', DecisionTreeClassifier(random_state=42))
            ]),
            {
                'decisiontreeclassifier__criterion': ['gini', 'entropy'],
                'decisiontreeclassifier__max_depth': [3, 5, 10],
                'decisiontreeclassifier__min_samples_split': [2, 5, 10],
                'decisiontreeclassifier__min_samples_leaf': [1, 2, 4],

            }
        ),
        "TabNet": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('tabnetclassifier', TabNetClassifier(seed=42, verbose=0))
            ]),
            {
                'tabnetclassifier__n_d': [8, 16],
                'tabnetclassifier__n_a': [8, 16],
                'tabnetclassifier__n_steps': [3, 5],
                'tabnetclassifier__gamma': [1.0, 1.3],
                'tabnetclassifier__lambda_sparse': [1e-3, 1e-4]
            }
        )
        ,
        "XGBOOST": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('xgbclassifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))
            ]),
            {
                "xgbclassifier__max_depth": [2, 3, 4],
                "xgbclassifier__n_estimators": [50, 100],
                "xgbclassifier__learning_rate": [0.01, 0.05, 0.1],
                "xgbclassifier__subsample": [0.7, 0.8, 0.9],
                "xgbclassifier__gamma": [0, 0.1, 0.2]
            }
        ),
        "LTCN": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('ltcn', LTCN())
            ]),
            {
                'ltcn__T': [10, 15, 20, 25, 30, 35],
                'ltcn__phi': [0.5, 0.8, 0.85, 0.90, 0.95],
                'ltcn__method': ['inverse', 'ridge'],
                'ltcn__function': ['sigmoid', 'tanh'],
                'ltcn__alpha': [0.0001, 0.001, 0.01]
            }
        ),
    }

    for model_name, (pipeline, param_grid) in model_defs.items():
        print(f"\nTraining and tuning {model_name}...")
        start_time = time.time()

        grid = GridSearchCV(pipeline, param_grid, cv=cv, scoring='f1_weighted', n_jobs=-1)
        grid.fit(X_train, y_train)

        end_time = time.time()
        model_times[model_name] = round(end_time - start_time, 2)
        models[model_name] = grid

        print(f"Cross-validation scores for {model_name}...")
        best_model = grid.best_estimator_
        cv_scores = cross_val_score(best_model, X_train, y_train, cv=cv, scoring='f1_weighted', n_jobs=-1)

        model_dir = os.path.join("results", model_name.replace(" ", "_"))
        os.makedirs(model_dir, exist_ok=True)

        cv_df = pd.DataFrame({"Fold": list(range(1, len(cv_scores) + 1)), "F1 Score": cv_scores})
        cv_df.to_csv(os.path.join(model_dir, "per_fold_f1_scores.csv"), index=False)

        plt.figure(figsize=(8, 6))
        bars = plt.bar(cv_df["Fold"], cv_df["F1 Score"], color='skyblue')
        plt.xlabel("Fold")
        plt.ylabel("F1 Score")
        plt.title(f"{model_name} - Per-Fold F1 Scores")
        plt.ylim(0, 1.0)
        for bar in bars:
            yval = bar.get_height()
            plt.text(bar.get_x() + bar.get_width() / 2.0, yval + 0.01, f"{yval:.2f}", ha='center', va='bottom')
        plt.tight_layout()
        plt.savefig(os.path.join(model_dir, "per_fold_f1_plot.png"))
        plt.close()

        val_mean_f1 = grid.best_score_
        val_std_f1 = grid.cv_results_['std_test_score'][grid.best_index_]
        grid.best_estimator_._val_mean_f1 = val_mean_f1
        grid.best_estimator_._val_std_f1 = val_std_f1

    for name, model in models.items():
        start_eval = time.time()
        evaluate_model_performance(
            name=name,
            model=model,
            X_train=X_train,
            y_train=y_train,
            X_test=X_test,
            y_test=y_test,
            feature_names=X_full.columns.tolist(),
            sens_test=sensitive_attr_test,
            output_dir="results",
            feature_importances_collector=all_feature_importance,
            fairness_collector=all_fairness,
            score_collector=all_model_scores
        )
        end_eval = time.time()
        evaluation_times[name] = round(end_eval - start_eval, 2)

    if all_model_scores:
        score_df = pd.DataFrame(all_model_scores)
        score_df.to_csv("results/model_scores_summary.csv", index=False)
        print("\nModel score summary saved to results/model_scores_summary.csv")
        print(score_df.to_string(index=False))

    train_df = pd.DataFrame(list(model_times.items()), columns=["Model", "Training Time (s)"])
    train_df.to_csv("results/model_training_times.csv", index=False)

    eval_df = pd.DataFrame(list(evaluation_times.items()), columns=["Model", "Evaluation Time (s)"])
    eval_df.to_csv("results/model_evaluation_times.csv", index=False)

    final_df = pd.DataFrame(all_model_scores)
    final_df["Training Time (s)"] = final_df["Model"].map(model_times)
    final_df["Evaluation Time (s)"] = final_df["Model"].map(evaluation_times)
    final_df.to_csv("results/model_overall_summary.csv", index=False)
    print("\nFinal Model Summary saved to results/model_overall_summary.csv")
    print(final_df.to_string(index=False))

    plt.figure(figsize=(10, 6))
    final_df_sorted = final_df.sort_values(by="Test F1", ascending=False)
    plt.bar(final_df_sorted["Model"], final_df_sorted["Test F1"])
    plt.title("Model Comparison by Test F1 Score")
    plt.ylabel("Test F1 Score")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig("results/model_comparison_plot.png")
    plt.show()
    plt.close()

def evaluate_model_performance(name, model, X_train, y_train, X_test, y_test, feature_names, sens_test,
                               output_dir="results", feature_importances_collector=None,
                               fairness_collector=None, score_collector=None, curve_data_collector=None):


    model_dir = os.path.join(output_dir, name.replace(" ", "_").replace("(", "").replace(")", ""))
    os.makedirs(model_dir, exist_ok=True)

    if isinstance(model, GridSearchCV):
        model = model.best_estimator_

    val_mean_f1 = getattr(model, "_val_mean_f1", None)
    val_std_f1 = getattr(model, "_val_std_f1", None)

    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    train_f1 = f1_score(y_train, y_train_pred, average='weighted')
    test_f1 = f1_score(y_test, y_test_pred, average='weighted')
    train_kappa = cohen_kappa_score(y_train, y_train_pred)
    test_kappa = cohen_kappa_score(y_test, y_test_pred)

    if score_collector is not None:
        score_data = {
            "Model": name,
            "Train F1": round(train_f1, 4),
            "Train Kappa": round(train_kappa, 4),
            "Test F1": round(test_f1, 4),
            "Test Kappa": round(test_kappa, 4)
        }
        if val_mean_f1 is not None:
            score_data["Validation F1"] = round(val_mean_f1, 4)
            score_data["Validation F1 Std"] = round(val_std_f1, 4)
        score_collector.append(score_data)

    try:
        y_score = model.predict_proba(X_test)[:, 1]
    except:
        try:
            y_score = model.decision_function(X_test)
        except:
            y_score = y_test_pred

    if curve_data_collector is not None:
        curve_data_collector[name] = {'y_true': y_test, 'y_score': y_score}

    plot_confusion(
        y_test, y_test_pred,
        title=f"{name} - Confusion Matrix",
        save_path=os.path.join(model_dir, "confusion_matrix.png")
    )
    plot_roc_curve(
        y_test, y_score,
        title=f"{name} - ROC Curve",
        save_path=os.path.join(model_dir, "roc_curve.png")
    )
    precision_recall(
        y_test, y_score,
        title=f"{name} - Precision-Recall Curve",
        save_path=os.path.join(model_dir, "precision_recall.png")
    )

    def get_ltcn_feature_importance(ltcn_model, feature_names):
        try:
            W = ltcn_model.W1
            importances = np.sum(np.abs(W), axis=1)
            return pd.DataFrame({
                "Feature": feature_names,
                "Importance": importances
            }).sort_values(by="Importance", ascending=False)
        except:
            return None

    def plot_feature_importance(df, model_name, save_path=None):
        df_sorted = df.sort_values("Importance", ascending=True)
        plt.figure(figsize=(8, 6))
        plt.barh(df_sorted["Feature"], df_sorted["Importance"])
        plt.xlabel("Importance")
        plt.title(f"Feature Importance - {model_name}")
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path)
        plt.close()

    try:
        results = {}
        ltcn_model = model
        if hasattr(model, 'named_steps') and 'ltcn' in model.named_steps:
            ltcn_model = model.named_steps['ltcn']

        if hasattr(ltcn_model, "W1"):
            native_df = get_ltcn_feature_importance(ltcn_model, feature_names)
            if native_df is not None:
                native_df.to_csv(os.path.join(model_dir, "native_importance.csv"), index=False)
                results["native"] = native_df
                plot_feature_importance(native_df, model_name=name,
                                        save_path=os.path.join(model_dir, "feature_importance.png"))
                if feature_importances_collector is not None:
                    for row in native_df.itertuples(index=False):
                        feature_importances_collector.append({
                            "Model": name,
                            "Feature": row.Feature,
                            "Importance": round(row.Importance, 5)
                        })

        additional_results = run_all_feature_importance_methods(
            model=model,
            X_train=X_train,
            X_test=X_test,
            y_train=y_train,
            y_test=y_test,
            feature_names=feature_names,
            model_name=name,
            output_dir=output_dir
        )
        results.update(additional_results)

    except Exception as e:
        print(f"Feature attribution error in {name}: {e}")

    try:
        fairness_dict = plot_fairness_bar(
            y_true=y_test,
            y_pred=y_test_pred,
            sensitive_attr=sens_test,
            group_labels=["Male", "Female"],
            save_path=os.path.join(model_dir, "fairness.png")
        )
        if fairness_collector is not None:
            for group, rate in fairness_dict.items():
                fairness_collector.append({
                    "Model": name,
                    "Group": group,
                    "Positive Prediction Rate": round(rate, 3)
                })
    except Exception as e:
        print(f"Fairness error in {name}: {e}")

evaluate_all_models_WITHOUT(X_full, y_full, sensitive_attr_column='gender')

def evaluate_all_models_with_training_noise(X_full, y_full, sensitive_attr_column='gender', noise_level=1.0):
    reset_random_seeds()

    all_feature_importance = []
    all_fairness = []
    all_model_scores = []

    X_train, X_test, y_train, y_test = train_test_split(
        X_full, y_full, test_size=0.2, random_state=42, stratify=y_full
    )

    sensitive_attr_train = X_train[sensitive_attr_column].values
    sensitive_attr_test = X_test[sensitive_attr_column].values

    def add_noise_to_features(X, noise_level=0.05, random_state=42):
        np.random.seed(random_state)
        X_noisy = X.copy()
        numeric_cols = X.select_dtypes(include=[np.number]).columns
        noise = np.random.normal(loc=0.0, scale=noise_level, size=X[numeric_cols].shape)
        X_noisy[numeric_cols] += noise
        return X_noisy

    print(f"\nAdding noise to training data (noise level = {noise_level})...")
    X_train_noisy = add_noise_to_features(X_train, noise_level=noise_level)

    models = {}
    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

    model_defs = {
        "Logistic Regression": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('smote', SMOTE(random_state=42)),
                ('logisticregression', LogisticRegression(max_iter=1000))
            ]),
            {
                "logisticregression__C": [0.01, 0.1, 1.0, 10.0, 100.0],
                "logisticregression__penalty": ['l2'],
                "logisticregression__solver": ['saga'],
            }

        ),
        "Random Forest": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('smote', SMOTE(random_state=42)),
                ('randomforestclassifier', RandomForestClassifier(random_state=42))
            ]),
            {
                "randomforestclassifier__n_estimators": [100, 200],
                "randomforestclassifier__max_depth": [5, 10, None],
                "randomforestclassifier__min_samples_split": [2, 5, 10],
                "randomforestclassifier__min_samples_leaf": [1, 2, 4],
            }
        ),
        "Decision Tree": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('smote', SMOTE(random_state=42)),
                ('decisiontreeclassifier', DecisionTreeClassifier(random_state=42))
            ]),
            {
                "decisiontreeclassifier__criterion": ['gini', 'entropy'],
                "decisiontreeclassifier__max_depth": [3, 5, 10],
                "decisiontreeclassifier__min_samples_split": [2, 5, 10],
                "decisiontreeclassifier__min_samples_leaf": [1, 2, 4],

            }
        ),
        "TabNet": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('smote', SMOTE(random_state=42)),
                ('tabnetclassifier', TabNetClassifier(seed=42, verbose=0))
            ]),
            {
                'tabnetclassifier__n_d': [8, 16],
                'tabnetclassifier__n_a': [8, 16],
                'tabnetclassifier__n_steps': [3, 5],
                'tabnetclassifier__gamma': [1.0, 1.3],
                'tabnetclassifier__lambda_sparse': [1e-3, 1e-4]
            }
        )

        ,
        "XGBOOST": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('smote', SMOTE(random_state=42)),
                ('xgbclassifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))
            ]),
            {
                "xgbclassifier__max_depth": [2, 3, 4],
                "xgbclassifier__n_estimators": [50, 100],
                "xgbclassifier__learning_rate": [0.01, 0.05, 0.1],
                "xgbclassifier__subsample": [0.7, 0.8, 0.9],
                "xgbclassifier__gamma": [0, 0.1, 0.2]
            }
        ),
        "LTCN": (
            Pipeline([
                ('scaler', StandardScaler()),
                ('smote', SMOTE(random_state=42)),
                ('ltcn', LTCN())
            ]),
            {'ltcn__T': [10, 15, 20, 25, 30, 35],
                'ltcn__phi': [0.5, 0.8, 0.85, 0.90, 0.95],
                'ltcn__method': ['inverse', 'ridge'],
                'ltcn__function': ['sigmoid', 'tanh'],
                'ltcn__alpha': [0.0001, 0.001, 0.01]

            }
        ),
    }

    for model_name, (pipeline, param_grid) in model_defs.items():
        print(f"\nTraining & tuning {model_name} on noisy data...")
        grid = GridSearchCV(pipeline, param_grid, cv=cv, scoring='f1_weighted', n_jobs=-1)
        grid.fit(X_train_noisy, y_train)
        models[model_name] = grid

        val_mean_f1 = grid.best_score_
        val_std_f1 = grid.cv_results_['std_test_score'][grid.best_index_]
        grid.best_estimator_._val_mean_f1 = val_mean_f1
        grid.best_estimator_._val_std_f1 = val_std_f1

    print("\nEvaluating models on clean test set...")
    for name, model in models.items():
        evaluate_model_performance(
            name=f"{name} (Trained on Noisy)",
            model=model,
            X_train=X_train_noisy,
            y_train=y_train,
            X_test=X_test,
            y_test=y_test,
            feature_names=X_full.columns.tolist(),
            sens_test=sensitive_attr_test,
            output_dir="results_noisy_train",
            feature_importances_collector=all_feature_importance,
            fairness_collector=all_fairness,
            score_collector=all_model_scores
        )

    if all_model_scores:
        score_df = pd.DataFrame(all_model_scores)
        score_df.to_csv("results_noisy_train/model_scores_summary.csv", index=False)
        print("Noisy training model score summary saved to results_noisy_train/model_scores_summary.csv")
        print(score_df.to_string(index=False))

    plt.show()
    plt.close()


def evaluate_model_performance(name, model, X_train, y_train, X_test, y_test, feature_names, sens_test,
                               output_dir="results", feature_importances_collector=None,
                               fairness_collector=None, score_collector=None, curve_data_collector=None):


    model_dir = os.path.join(output_dir, name.replace(" ", "_").replace("(", "").replace(")", ""))
    os.makedirs(model_dir, exist_ok=True)

    if isinstance(model, GridSearchCV):
        model = model.best_estimator_

    val_mean_f1 = getattr(model, "_val_mean_f1", None)
    val_std_f1 = getattr(model, "_val_std_f1", None)

    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    train_f1 = f1_score(y_train, y_train_pred, average='weighted')
    test_f1 = f1_score(y_test, y_test_pred, average='weighted')
    train_kappa = cohen_kappa_score(y_train, y_train_pred)
    test_kappa = cohen_kappa_score(y_test, y_test_pred)

    if score_collector is not None:
        score_data = {
            "Model": name,
            "Train F1": round(train_f1, 4),
            "Train Kappa": round(train_kappa, 4),
            "Test F1": round(test_f1, 4),
            "Test Kappa": round(test_kappa, 4)
        }
        if val_mean_f1 is not None:
            score_data["Validation F1"] = round(val_mean_f1, 4)
            score_data["Validation F1 Std"] = round(val_std_f1, 4)
        score_collector.append(score_data)

    try:
        y_score = model.predict_proba(X_test)[:, 1]
    except:
        try:
            y_score = model.decision_function(X_test)
        except:
            y_score = y_test_pred

    if curve_data_collector is not None:
        curve_data_collector[name] = {'y_true': y_test, 'y_score': y_score}

    plot_confusion(
        y_test, y_test_pred,
        title=f"{name} - Confusion Matrix",
        save_path=os.path.join(model_dir, "confusion_matrix.png")
    )
    plot_roc_curve(
        y_test, y_score,
        title=f"{name} - ROC Curve",
        save_path=os.path.join(model_dir, "roc_curve.png")
    )
    precision_recall(
        y_test, y_score,
        title=f"{name} - Precision-Recall Curve",
        save_path=os.path.join(model_dir, "precision_recall.png")
    )

    def get_ltcn_feature_importance(ltcn_model, feature_names):
        try:
            W = ltcn_model.W1
            importances = np.sum(np.abs(W), axis=1)
            return pd.DataFrame({
                "Feature": feature_names,
                "Importance": importances
            }).sort_values(by="Importance", ascending=False)
        except:
            return None

    def plot_feature_importance(df, model_name, save_path=None):
        df_sorted = df.sort_values("Importance", ascending=True)
        plt.figure(figsize=(8, 6))
        plt.barh(df_sorted["Feature"], df_sorted["Importance"])
        plt.xlabel("Importance")
        plt.title(f"Feature Importance - {model_name}")
        plt.tight_layout()
        if save_path:
            plt.savefig(save_path)
        plt.close()

    try:
        results = {}
        ltcn_model = model
        if hasattr(model, 'named_steps') and 'ltcn' in model.named_steps:
            ltcn_model = model.named_steps['ltcn']

        if hasattr(ltcn_model, "W1"):
            native_df = get_ltcn_feature_importance(ltcn_model, feature_names)
            if native_df is not None:
                native_df.to_csv(os.path.join(model_dir, "native_importance.csv"), index=False)
                results["native"] = native_df
                plot_feature_importance(native_df, model_name=name,
                                        save_path=os.path.join(model_dir, "feature_importance.png"))
                if feature_importances_collector is not None:
                    for row in native_df.itertuples(index=False):
                        feature_importances_collector.append({
                            "Model": name,
                            "Feature": row.Feature,
                            "Importance": round(row.Importance, 5)
                        })

        additional_results = run_all_feature_importance_methods(
            model=model,
            X_train=X_train,
            X_test=X_test,
            y_train=y_train,
            y_test=y_test,
            feature_names=feature_names,
            model_name=name,
            output_dir=output_dir
        )
        results.update(additional_results)

    except Exception as e:
        print(f"Feature attribution error in {name}: {e}")

    try:
        fairness_dict = plot_fairness_bar(
            y_true=y_test,
            y_pred=y_test_pred,
            sensitive_attr=sens_test,
            group_labels=["Male", "Female"],
            save_path=os.path.join(model_dir, "fairness.png")
        )
        if fairness_collector is not None:
            for group, rate in fairness_dict.items():
                fairness_collector.append({
                    "Model": name,
                    "Group": group,
                    "Positive Prediction Rate": round(rate, 3)
                })
    except Exception as e:
        print(f"Fairness error in {name}: {e}")
evaluate_all_models_with_training_noise(X_full=X_full, y_full=y_full, sensitive_attr_column="gender", noise_level=1.0)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
from google.colab import files
from google.colab import drive
drive.mount('/content/drive')


# === Load and Preprocess Data ===
data_path = '/content/drive/My Drive/Colab Notebooks/train.csv'
data = pd.read_csv(data_path)

features_train = data.drop(columns=['Class/ASD', 'ID'])
target_train = data['Class/ASD']
combined_features = features_train.copy()

# Label encode categorical features
combined_features['ethnicity'] = combined_features['ethnicity'].str.strip().str.lower().replace({'?': np.nan, 'others': 'other'})
most_common_ethnicity = combined_features['ethnicity'].mode()[0]
combined_features['ethnicity'] = combined_features['ethnicity'].fillna(most_common_ethnicity)
le_ethnicity = LabelEncoder()
combined_features['ethnicity'] = le_ethnicity.fit_transform(combined_features['ethnicity'])

le_country = LabelEncoder()
combined_features['contry_of_res'] = le_country.fit_transform(combined_features['contry_of_res'])

binary_map = {'no': 0, 'yes': 1}
gender_map = {'f': 0, 'm': 1}
combined_features['used_app_before'] = combined_features['used_app_before'].map(binary_map)
combined_features['jaundice'] = combined_features['jaundice'].map(binary_map)
combined_features['austim'] = combined_features['austim'].map(binary_map)
combined_features['gender'] = combined_features['gender'].map(gender_map)

if 'age_desc' in combined_features.columns:
    combined_features.drop('age_desc', axis=1, inplace=True)

combined_features['relation'] = combined_features['relation'].replace({'?': np.nan, 'Others': 'other'})
most_common_relation = combined_features['relation'].mode()[0]
combined_features['relation'] = combined_features['relation'].fillna(most_common_relation)
le_relation = LabelEncoder()
combined_features['relation'] = le_relation.fit_transform(combined_features['relation'])

X_full = combined_features
y_full = target_train

# === Plotting ===
sns.set(style="whitegrid")

# 1. Class Distribution
sns.countplot(x=y_full.map({0: 'Neurotypical', 1: 'ASD'}))
plt.title("ASD Diagnosis Distribution")
plt.savefig("ASD_Diagnosis_Distribution.png")
plt.clf()

# 2. PCA Projection
X_scaled = StandardScaler().fit_transform(X_full.select_dtypes(include=np.number))
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_full.map({0: 'Neurotypical', 1: 'ASD'}))
plt.title("PCA Projection by ASD Class")
plt.savefig("output-21.png")
plt.clf()

# 3. Psychometric Score Distribution
sns.kdeplot(data=X_full, x="result", hue=y_full.map({0: 'Neurotypical', 1: 'ASD'}), fill=True)
plt.title("Psychometric Score Distribution")
plt.savefig("Psychometric_Result_Distribution_by_ASD_Class_Optimized.png")
plt.clf()

# 4. Behavioral Indicators
indicator_cols = [col for col in X_full.columns if 'A' in col and '_Score' in col]
for col in indicator_cols:
    sns.barplot(x=y_full.map({0: 'NT', 1: 'ASD'}), y=X_full[col])
    plt.title(col)
    plt.savefig(f"Indicator_{col}.png")
    plt.clf()

plt.figure(figsize=(20, 10))  # Adjust width and height as needed
sns.heatmap(X_full.corr(), cmap='coolwarm', annot=True, fmt=".2f")
plt.title("Correlation Heatmap")
plt.savefig("Correlation_Matrix_Large.png", bbox_inches='tight')
plt.clf()

# 6. Demographics
sns.countplot(x=X_full["gender"].map({0: "Female", 1: "Male"}), hue=y_full.map({0: "NT", 1: "ASD"}))
plt.title("Gender by ASD Class")
plt.savefig("Gender_by_ASD_Class.png")
plt.clf()

sns.histplot(x=X_full['age'], hue=y_full.map({0: "NT", 1: "ASD"}), kde=True)
plt.title("Age Distribution by ASD Class")
plt.savefig("Age Distribution By ASD Class.png")
plt.clf()

eth_df = X_full.copy()
eth_map = dict(zip(le_ethnicity.transform(le_ethnicity.classes_), le_ethnicity.classes_))
eth_df['ethnicity'] = eth_df['ethnicity'].map(eth_map)
eth_df['ASD'] = y_full.map({0: "NT", 1: "ASD"})
sns.countplot(data=eth_df, x="ethnicity", hue="ASD")
plt.title("Ethnicity by ASD Class")
plt.xticks(rotation=45)
plt.savefig("Ethnicity_by_ASD_Class.png")
plt.clf()

rel_df = X_full.copy()
rel_map = dict(zip(le_relation.transform(le_relation.classes_), le_relation.classes_))
rel_df['relation'] = rel_df['relation'].map(rel_map)
rel_df['ASD'] = y_full.map({0: "NT", 1: "ASD"})
sns.countplot(data=rel_df, x="relation", hue="ASD")
plt.title("Relation by ASD Class")
plt.xticks(rotation=45)
plt.savefig("Relation_by_ASD_Class.png")
plt.clf()

# Clinical features
for col, label in zip(['used_app_before', 'jaundice', 'austim'], ['Used_App_Before', 'Jaundice', 'Family_Autism_History']):
    sns.countplot(x=X_full[col].map({0: "No", 1: "Yes"}), hue=y_full.map({0: "NT", 1: "ASD"}))
    plt.title(f"{label} by ASD Class")
    plt.savefig(f"{label}_by_ASD_Class.png")
    plt.clf()

from scipy.stats import ttest_ind

def compute_cohens_d(data, target_column='Class/ASD'):
    classes = data[target_column].unique()
    if len(classes) != 2:
        raise ValueError("Cohen's d is defined for two groups only.")

    group1 = data[data[target_column] == classes[0]]
    group2 = data[data[target_column] == classes[1]]

    cohen_data = []
    for col in data.columns:
        if col != target_column and np.issubdtype(data[col].dtype, np.number):
            m1, m2 = group1[col].mean(), group2[col].mean()
            s1, s2 = group1[col].std(), group2[col].std()
            n1, n2 = len(group1), len(group2)
            pooled_std = np.sqrt(((n1-1)*s1**2 + (n2-1)*s2**2) / (n1+n2-2))
            d = (m1 - m2) / pooled_std if pooled_std > 0 else 0
            cohen_data.append((col, m1, m2, d))

    cohen_df = pd.DataFrame(cohen_data, columns=["Feature", "Mean (Group 0)", "Mean (Group 1)", "Cohen's d"])
    cohen_df["Abs d"] = np.abs(cohen_df["Cohen's d"])
    cohen_df.sort_values(by="Abs d", ascending=False, inplace=True)
    return cohen_df
# Combine features and target for analysis
combined_for_stats = combined_features.copy()
combined_for_stats['Class/ASD'] = y_full

cohen_results = compute_cohens_d(combined_for_stats)
cohen_results.to_csv("cohen_d_effect_sizes.csv", index=False)

# Show top 10
cohen_results.head(10)
plt.figure(figsize=(10, 6))
top_d = cohen_results.head(12)
sns.barplot(data=top_d, y="Feature", x="Abs d", palette="viridis")
plt.axvline(0.2, color='gray', linestyle='--', label='Small')
plt.axvline(0.5, color='blue', linestyle='--', label='Medium')
plt.axvline(0.8, color='red', linestyle='--', label='Large')
plt.xlabel("Cohen's d (absolute)")
plt.ylabel("Feature")
plt.title("Top Features by Cohen's d Effect Size")
plt.legend()
plt.tight_layout()
plt.savefig("cohen_d_barplot.png", dpi=300)
plt.show()

# Combine features and labels
df = combined_features.copy()
df['ASD'] = y_full

# Behavioral columns (adjust if needed)
behavioral_cols = [col for col in df.columns if "A" in col and "_Score" in col]

# Group by ASD class and calculate mean (i.e., response rate per feature)
behavioral_means = df.groupby("ASD")[behavioral_cols].mean().T
behavioral_means.columns = ["Neurotypical", "ASD"]
behavioral_means.plot(kind='bar', figsize=(10, 6))

plt.title("Behavioral indicator response frequencies by ASD status")
plt.ylabel("Average Response")
plt.xlabel("Behavioral Feature")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig("output-31.png", dpi=300)
plt.show()

# Combine features and target
df = combined_features.copy()
df['ASD'] = y_full

# Select behavioral indicators
behavioral_cols = [col for col in df.columns if col.startswith("A") and "_Score" in col]

# Calculate group means
behavioral_means = df.groupby("ASD")[behavioral_cols].mean().T
behavioral_means.columns = ["Neurotypical", "ASD"]
behavioral_means = behavioral_means.reset_index().rename(columns={'index': 'Feature'})

# Round for clean LaTeX table
behavioral_means = behavioral_means.round(3)

# Save as CSV (optional)
behavioral_means.to_csv("behavioral_means_table.csv", index=False)

# Convert to LaTeX
latex_table = behavioral_means.to_latex(index=False, caption="Mean behavioral indicator responses by ASD status.", label="tab:behavioral-means")
print(latex_table)

from scipy.stats import ttest_ind

def cohen_d_feature_table(X, y):
    data = X.copy()
    data["Class/ASD"] = y

    group_asd = data[data["Class/ASD"] == 1]
    group_nt = data[data["Class/ASD"] == 0]

    rows = []
    for col in X.columns:
        if np.issubdtype(X[col].dtype, np.number):
            mean_asd = group_asd[col].mean()
            mean_nt = group_nt[col].mean()
            std_asd = group_asd[col].std()
            std_nt = group_nt[col].std()
            n_asd = len(group_asd)
            n_nt = len(group_nt)

            pooled_std = np.sqrt(((n_asd - 1) * std_asd ** 2 + (n_nt - 1) * std_nt ** 2) / (n_asd + n_nt - 2))
            d = (mean_asd - mean_nt) / pooled_std if pooled_std != 0 else 0

            rows.append([col, round(mean_asd, 3), round(mean_nt, 3), round(d, 3)])

    result_df = pd.DataFrame(rows, columns=["Feature", "Mean (ASD)", "Mean (NT)", "Cohen's d"])
    result_df = result_df.sort_values(by="Cohen's d", ascending=False)
    return result_df

# Run
cohen_df = cohen_d_feature_table(combined_features, y_full)
cohen_df.to_csv("cohen_effect_table.csv", index=False)
cohen_df.head(12)

